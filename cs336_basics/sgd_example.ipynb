{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36020a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Callable, Iterable\n",
    "from typing import Optional\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3):\n",
    "        # if lr < 0:\n",
    "        #     raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        defaults = {\"lr\": lr}\n",
    "        super().__init__(params, defaults)\n",
    "    def step(self, closure: Optional[Callable] = None):\n",
    "        loss = None if closure is None else closure()\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"] \n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                state = self.state[p] \n",
    "                t = state.get(\"t\", 0) \n",
    "                grad = p.grad.data \n",
    "                p.data -= lr / math.sqrt(t + 1) * grad \n",
    "                state[\"t\"] = t + 1 \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d57a0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.163795471191406\n",
      "28.0089111328125\n",
      "27.222299575805664\n",
      "26.597261428833008\n",
      "26.067970275878906\n",
      "25.603740692138672\n",
      "25.187339782714844\n",
      "24.8079833984375\n",
      "24.458383560180664\n",
      "24.133359909057617\n",
      "23.829057693481445\n",
      "23.542539596557617\n",
      "23.271474838256836\n",
      "23.014019012451172\n",
      "22.768644332885742\n",
      "22.534099578857422\n",
      "22.3093204498291\n",
      "22.093412399291992\n",
      "21.885604858398438\n",
      "21.685230255126953\n",
      "21.49170684814453\n",
      "21.304519653320312\n",
      "21.12322235107422\n",
      "20.947410583496094\n",
      "20.776723861694336\n",
      "20.610841751098633\n",
      "20.449474334716797\n",
      "20.2923583984375\n",
      "20.139251708984375\n",
      "19.989938735961914\n",
      "19.844219207763672\n",
      "19.7019100189209\n",
      "19.562843322753906\n",
      "19.426862716674805\n",
      "19.293821334838867\n",
      "19.163593292236328\n",
      "19.036048889160156\n",
      "18.911075592041016\n",
      "18.788562774658203\n",
      "18.668413162231445\n",
      "18.55052947998047\n",
      "18.434825897216797\n",
      "18.32122039794922\n",
      "18.209630966186523\n",
      "18.099987030029297\n",
      "17.992223739624023\n",
      "17.886266708374023\n",
      "17.782060623168945\n",
      "17.679546356201172\n",
      "17.578662872314453\n",
      "17.47936248779297\n",
      "17.3815975189209\n",
      "17.28531265258789\n",
      "17.190471649169922\n",
      "17.097026824951172\n",
      "17.00493621826172\n",
      "16.914161682128906\n",
      "16.824668884277344\n",
      "16.73641586303711\n",
      "16.64937400817871\n",
      "16.563505172729492\n",
      "16.478784561157227\n",
      "16.395179748535156\n",
      "16.312660217285156\n",
      "16.231199264526367\n",
      "16.150768280029297\n",
      "16.071348190307617\n",
      "15.992905616760254\n",
      "15.915422439575195\n",
      "15.838875770568848\n",
      "15.763240814208984\n",
      "15.688501358032227\n",
      "15.614631652832031\n",
      "15.541613578796387\n",
      "15.469430923461914\n",
      "15.398062705993652\n",
      "15.327493667602539\n",
      "15.257704734802246\n",
      "15.188677787780762\n",
      "15.120401382446289\n",
      "15.0528564453125\n",
      "14.986027717590332\n",
      "14.919904708862305\n",
      "14.854469299316406\n",
      "14.789709091186523\n",
      "14.72561264038086\n",
      "14.662164688110352\n",
      "14.599355697631836\n",
      "14.537169456481934\n",
      "14.475598335266113\n",
      "14.414628028869629\n",
      "14.354248046875\n",
      "14.294447898864746\n",
      "14.235219955444336\n",
      "14.176549911499023\n",
      "14.118431091308594\n",
      "14.06085205078125\n",
      "14.003802299499512\n",
      "13.947275161743164\n",
      "13.891263008117676\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1)\n",
    "for t in range(100):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0eeadfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.669509887695312\n",
      "18.348485946655273\n",
      "13.525718688964844\n",
      "10.58243179321289\n",
      "8.571769714355469\n",
      "7.106978893280029\n",
      "5.9937944412231445\n",
      "5.121867656707764\n",
      "4.423135757446289\n",
      "3.8530426025390625\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1e1)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84cb5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.986722946166992\n",
      "24.98672103881836\n",
      "4.28704309463501\n",
      "0.10259860754013062\n",
      "9.912209835852357e-17\n",
      "1.1047772562469953e-18\n",
      "3.7201740422407775e-20\n",
      "2.2161331833032014e-21\n",
      "1.9011422860258998e-22\n",
      "2.1123801775646166e-23\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1e2)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd5a6b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.053892135620117\n",
      "10127.4541015625\n",
      "1749170.75\n",
      "194576576.0\n",
      "15760702464.0\n",
      "994680700928.0\n",
      "51063654711296.0\n",
      "2196977106288640.0\n",
      "8.097588746964173e+16\n",
      "2.6002254278007194e+18\n"
     ]
    }
   ],
   "source": [
    "weights = torch.nn.Parameter(5 * torch.randn((10, 10)))\n",
    "opt = SGD([weights], lr=1e3)\n",
    "for t in range(10):\n",
    "    opt.zero_grad() # Reset the gradients for all learnable parameters.\n",
    "    loss = (weights**2).mean() # Compute a scalar loss value.\n",
    "    print(loss.cpu().item())\n",
    "    loss.backward() # Run backward pass, which computes gradients.\n",
    "    opt.step() # Run optimizer step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
